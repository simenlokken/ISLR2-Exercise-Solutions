---
title: "Chapter 3 - Linear Regression - Exercises"
format:
  html:
    warning: false
    message: false
editor: visual
---

### Set workspace

```{r, warning=FALSE, message=FALSE}

library(ISLR2)
library(tidyverse)
library(broom)
library(gridExtra)
library(GGally)
library(ggcorrplot)
library(janitor)

theme_set(theme_minimal())
```

### Exercise 8)

(a).

```{r, warning=FALSE, message=FALSE}

Auto |> view()

# First, some plotting

Auto |> 
  ggplot(aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Fit model

auto_model <- lm(mpg ~ horsepower, data = Auto)

summary(auto_model)

# Predictions

predict(auto_model,
        tibble(horsepower = c(98)),
        interval = "confidence")

predict(auto_model,
        tibble(horsepower = c(98)),
        interval = "prediction")
```

i-iii. It is clear from our plot there is a relationship between miles per gallon and horsepower; as horsepower increases miles per gallon decreases. Our regression result say the same; for each increase in horsepower, there is a - 0.158 decrease in miles per gallon. The relationship is negative.

iv\. The predicted miles per gallon usage based on our model with a horsepower of 98, is 24.5 (95 % CI: \[23.97, 24,96\]; PI: \[14.81, 34.12\]).

(b).

```{r, warning=FALSE, message=FALSE}

Auto |> 
  ggplot(aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

(c).

```{r}

# Source functions for model plots

source("model_plots_linear_regression.R")

augmented_auto_model <- augment(auto_model)

# Resdiual plot

res <- augmented_auto_model |> 
  ggplot(aes(x = .fitted, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    labs(title = "Residual plot")

# Q-Q plot

qq <- augmented_auto_model |> 
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "Q-Q plot")

# Cooks's Distance plot

cooks <- augmented_auto_model |> 
  ggplot(aes(x = .fitted, y = .cooksd)) +
  geom_point() +
  labs(title = "Cook's Distance plot")

grid.arrange(res, qq, cooks, nrow = 1)
```

The main thing I'm concerned about from these diagnostic plots is the curvature shown in the residual plot. It may indicate that a polynomial regression is more suitable than a linear regression for these data.

#### Exercise 9

(a)-(b).

```{r, warning=FALSE, message=FALSE}

Auto |> 
  select(- origin, - name) |>
  ggpairs()
```

(c).

```{r}

multiple_auto_model <- 
  lm(mpg ~ . - name, data = Auto)

summary(multiple_auto_model)
```

i\. Yes, but they impact the response differently. For example, while number of cylinders have a negative relationship with miles per gallon, origin have a positive relationship.

ii\. Displacement, weight, year and origin have a statistically significant relationship with miles per gallon.

iii\. The coefficient of the year variable suggests that newer cars are more fuel economical than older cars.

#### Exercise 10)

(a).

```{r}

Carseats |> view()

# Clean the names to make the data easier to work with

carseats <- Carseats |> clean_names()

# Fit model

multiple_carseats_model <- 
  lm(sales ~ price + urban + us, data = carseats)

summary(multiple_carseats_model)
```

(b).

1\) Price (in thousands). The model indicates that price affect sales of car seats negatively, i.e., when sales increase when the price goes down.

2\) Urban (qualitative, yes/no, whether the store is in an urban or rural location. The model indicates car seats sales decrease if the store is located in an urban location. That makes sense because the necessity of a car is normally higher in rural areas due to bigger distances and lack of public transport.

3\) US (qualitative, yes/no, whether the store is in the US or not). The model indicates that stores located in the US sell more car seats than stores located outside of the US.

(c).

Not suited for a programming solution.

(d).

Price and US is statistically significant at the less than 0.001 level, so if we can reject H0 and conclude that those variables do affect sales.

(e).

```{r}

# Fit new model

new_multiple_carseats_model <- 
  lm(sales ~ price + us, data = carseats)

summary(new_multiple_carseats_model)
```

(f).

I will only consider the last model, model E). The RSE is 2.47, meaning that the sales deviate by 2470 car seats on average in our model. The R squared is 0.24, meaning that our model explains 24 per cent of the variance, i.e., there is 76 per cent that is still unexplained. The F statistic is highly statistically significant so price and US is associated with sales. Altogether, the model does explain some of the variance and we can be quite confident that price and US is associated with sales but there is for sure a lot we do not know about predicting sales of car seats.

(g).

```{r}

confint(new_multiple_carseats_model)
```

(h).

For checking leverage outliers, we will calculate the studentized residuals and check if is \> 3 with a studres() from the MASS package.

```{r}

augmented_new_multiple_carseats_model <- augmented_new_multiple_carseats_model |> 
  mutate(studentized_resid = studres(new_multiple_carseats_model))

augmented_new_multiple_carseats_model |> 
  ggplot(aes(x = .fitted, y = studentized_resid)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "blue") +
  geom_hline(yintercept = -3, color = "blue")
```

Even though some observations are close to an absolute value of 3, no observation is bigger. There are no outliers to worry about in our model. We must also check for high leverage points. Since the augment() function calculates .hat values for us, we can just plot them.

```{r}

augmented_new_multiple_carseats_model |> 
  ggplot(aes(x = .fitted, y = .hat)) +
  geom_col(width = 0.05)
```

We see that our model has one observations that is clearly bigger than the rest. We can compare its size to the average leverage for all observations. ISLR 2 states that we should care about high leverage points if they greatly exceed the average leverage, i.e., x \> (p + 1) / n.

```{r}

hat_value <- max(augmented_new_multiple_carseats_model$hat_values) # Highest hat value
predictors <- 2 # Number of predictors in our model
num_observations <- 400 # Number of model observations
avg_leverage <- (predictors + 1) / num_observations

if (hat_value > avg_leverage) {
  print("There might be a high leverage point to assess further")
  
} else {
  print("There is no high leverage point to worry about")
}

# Difference between average leverage and our potential high point

abs(hat_value - avg_leverage)
```

If I were to continue this analysis, I would consider to remove the point in question.

#### Exercise 13)

(a)-(c).

```{r}

x <- c(rnorm(100, 0, 1))

eps <- c(rnorm(100, 0, 0.25))

y <- - 1 + 0.5 * x + eps

length(y)
```

The length of y is 100, the value of beta 0 is -1 and beta 1 is 0.5.

(d).

```{r}

x_y <- tibble(x = x, y = y)

x_y |>
  ggplot(aes(x, y)) +
  geom_point()
```

I observe a linear relationship.

(e).

```{r}

x_y_linear_model <- lm(y ~ x, data = x_y)
```

The model is not quite similar to the model to the equation we called y. The intercept is almost the same, but beta 1 is of by 0.325 units.

(f).

```{r}

x_y |>
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(intercept = -1, slope = 0.5) +
  geom_abline(intercept = -1.0047, slope = 0.5325, color = "red")
```

#### Exercise 15)

\(a\)

```{r}

summary(lm_boston_zn <- lm(crim ~ zn, data = Boston))
summary(lm_boston_indus <- lm(crim ~ indus, data = Boston))
summary(lm_boston_chas <- lm(crim ~ chas, data = Boston))
summary(lm_boston_nox <- lm(crim ~ nox, data = Boston))
summary(lm_boston_rm <- lm(crim ~ rm, data = Boston))
summary(lm_boston_age <- lm(crim ~ age, data = Boston))
summary(lm_boston_dis <- lm(crim ~ dis, data = Boston))
summary(lm_boston_rad <- lm(crim ~ rad, data = Boston))
summary(lm_boston_tax <- lm(crim ~ tax, data = Boston))
summary(lm_boston_ptratio <- lm(crim ~ ptratio, data = Boston))
summary(lm_boston_lstat <- lm(crim ~ lstat, data = Boston))
summary(lm_boston_medv <- lm(crim ~ medv, data = Boston))
```

All predictors are statistically significant above the often used 0.05 level except for "chas", i.e., a dummy variable for stating whether the tract bounds river or not.

(b).

```{r}

# Fit multiple linear regression model

lm_boston_multiple <- lm(crim ~ ., data = Boston)

summary(lm_boston_multiple)
```

The null hypothesis states that there is no statistically significant effect of a given predictor on the response variable "crim". The model show that we can reject the null hypothesis for "zn", "dis", "rad" and "medv" and conclude that these variables do have an affect.

However, we should not blindly dismiss some variables for having an effect on the response based on the p-value alone. For instance, the predictor "nox" have a large effect magnitude on the response and it just slightly misses the alpha = 0.05 threshold.

(c).

The result from (a) differs from (b). In (a), all predictors were statistically significant except "chad". In (b), there was only four predictors that were statistically significant.

```{r}

# Plotting univariate coefficients against multivariate coefficients

tidy_lm_boston_multiple <- tidy(lm_boston_multiple)
```
